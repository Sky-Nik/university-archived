Особливості структури в $f$, $g$, $A$, і $B$ зачасту можна використати аби виконувати кроки оновлення змінних $x$ та $z$ ефективніше. Тут ми розглянемо три загальні випадки які часто зустрічатимуться нам в подальшому: квадратичні цільові доданки, розділимі цільові функції та розділимі обмеження, а також гладкі цільові доданки. Наше обговорення буде описане у термінах кроку оновлення змінної $x$, але аналогічно переноситься на крок оновлення змінної $z$. Записуватимемо крок оновлення змінної $x$ як

\begin{equation}
	x^+ = \argmin_x \left(f(x) + (\rho / 2) \cdot \|A x - v\|_2^2\right),
\end{equation}

де $v = - B z + c - u$ --- відомий сталий (на кроці оновлення змінної $x$) 
вектор.

\subsection{Проксимальний оператор}

По-перше, розглянемо найпростіший випадок коли $A = I$, який насправді
доволі часто зустрічається у прикладах. Тоді оновлення $x$ набуває вигляду:

\begin{equation}
	x^+ = \argmin_x \left(f(x) + (\rho / 2) \cdot \|x - v\|_2^2\right).
\end{equation}

\begin{definition}
	Як функція $v$ права частина цієї рівності позначається $\prox_{f,\rho}(v)$ і називається \textit{проксимальним оператором} функції $f$ зі штрафним доданком $\rho$\cite{127}.
\end{definition}

\begin{definition}
	У варіаційному аналізі,
	
	\begin{equation}
		\tilde f(v) = \inf_x \left(f(x) + (\rho / 2) \cdot \|x - v\|_2^2\right)
	\end{equation}
	
	відома як \textit{обгортка Мореа} або \textit{регуляризація Мореа-Йосіди} функції $f$ і є тісно пов'язаною з теорією алгоритму проксимальної точки \cite{144}.
\end{definition}

\begin{definition}
	Мінімізацію по $x$ у проксимальному операторі зазвичай називають \textit{проксимальною мінімізацією}. 
\end{definition}

Хоча ці спостереження самі по собі не покращують ефективність ADMM, але вони по\-в'яз\-у\-ють цей крок алгоритму з іншими відомими ідеям. \medskip

Коли функція $f$ достатньо проста, то проксимальний оператор (тобто оновлення $x$) може бути обчисленим аналітично, див. \cite{41} для великої кількості прикладів.

\begin{example}
	Зокрема, коли $f$ є індикатором замкненої непорожньої опуклої множини $\mathcal{C}$, то крок оновлення змінної $x$ набуває вигляду
	
	\begin{equation}
		x^+ = \argmin_x \left(f(x) + (\rho / 2) \cdot \|x - v\|_2^2\right) = \Pi_\mathcal{C}(v),
	\end{equation}
	де $\Pi_\mathcal{C}$ позначає проекцію (в Евклідовій нормі) на $\mathcal{C}$.
\end{example}

\begin{remark}
	Попереднє твердження справджується не залежно від вибору $\rho$, тобто автоматично гарно поєднується з варіаціями у яких $\rho$ змінне, як у \S3.4.1.
\end{remark}

\begin{example}
	Зокрема, якщо $f$ --- індикаторна функція невід'ємного ортанту $\RR_+^n$, то ми маємо $x^+ = (v)_+$, тобто вектор який отримується взяттям невід'ємної частини кожної компоненти вектору $v$.
\end{example}

\subsection{Квадратичні штрафні доданки}

Припустимо, що $f$ --- задана (опукла) квадратична функція

\begin{equation}
	f(x) = (1 / 2) \cdot x^\intercal P x + q^\intercal x + r,
\end{equation}

де $P \in \mathbb{S}_+^n$, тобто симетрична додатно-визначена матриця $n \times n$.

\begin{remark}
	Це охоплює випадки коли $f$ --- лінійна або константа, за рахунок використання або $P = 0$, або $q = 0$, або і того і того.
\end{remark}

Тоді, у припущенні існування оберненої у $P + \rho A^\intercal A$, $x^+$ є афінною функцією змінної $v$, а саме

\begin{equation}
	\label{eq:4.1}
	x^+ - \left(P + \rho A^\intercal A\right)^{-1} \cdot \left(\rho A^\intercal v - q\right).
\end{equation} 

Іншими словами, оновлення $x$ зводиться до розв'язування СЛАР з додатно-визначеною матрицею коефіцієнтів $P + \rho A^\intercal A$ і правою частиною $\rho A^\intercal v - q$. Як було показано раніше, розумне використання лінійної алгебри може експлуатувати цей факт і суттєво покращити роботу. Для загального бекграунду з чисельної лінійної алгебри див. \cite{47} або \cite{90}. Див.~\cite[додаток C]{20} для короткого огляду аналітичних методів.

\subsubsection{Аналітичні методи}

Поки що розглянемо використання аналітичних методів для роз\-в'яз\-у\-ван\-ня  вищезгаданої системи лінійних алгебраїчних рівнянь, ітеративні методи будуть оглянуті в \S4.3. Аналітичні методи розв'язування СЛАР $F x = g$ базуються на розкладі (\textit{факторизації}) матриці $F$ у добуток  $F_1 \cdot F_2 \cdot \ldots \cdot F_k$ простіших матриць, а потім обчислення  $x = F^{-1} b$ шляхом розв'язання низки задач вигляду $F_i z_i = z_{i - 1}$, де $z_1 = F_1^{-1} g$ і $x = z_k$. 

\begin{definition}
	Процес розв'язування простіших задач називається \textit{зворотнім ходом}.
\end{definition}

Обчислювальна складність факторизації і зворотнього ходу залежить від розрідженості та інших властивостей матриці $F$. Як наслідок, сумарна складність розв'язування системи $F x = g$ є сумою складностей факторизації $F$ та оберненого ходу. \medskip

У нашому випадку матриця коефіцієнтів $F = P + \rho A^\intercal A$ і права частина $g = \rho A^\intercal v - q$, де $P \in \mathbb{S}_+^n$ і $A \in \RR^{p \times n}$. Припустимо, що ми не експлуатуємо структуру $A$ чи $P$, тобто використовуємо загальні методи що спрацюють для довільної матриці. Тоді створення матриці $F = P + \rho A^\intercal A$ вимагає $O\left(pn^2\right)$ флопс. Факторизація Холецького матриці $F$ здійснюється за $O\left(n^3\right)$ операцій, а зворотній хід є $O(n^2)$. \quad (Вартість створення $g$ нікчемно мала у порівнянні з вище перерахованими). \medskip

Резюмуючи, коли $p$ приблизно $n$ або більше, то сумарна складність $O\left(p n^2\right)$. \quad (Коли $p$ менше ніж $n$ на порядок то лема про обернену матрицю описана нижче дозволяє провести крок увесь оновлення за $O\left(p^2 n\right)$ флопс).

\subsubsection{Експлуатація розрідженості}

Коли $A$ і $P$ такі, що $F$ розріджена (тобто має достатньо нульових елементів для експлуатації їх наявності), то можна застосувати ефективніші процедури факторизації і зворотного ходу.

\begin{example}
    Якщо і $P$ і $A$ діагональні матриці $n \times n$ то як факторизація так і зворотній хід можуть бути виконані за $O(n)$.
\end{example}

\begin{example}
    Якщо і $P$ і $A$ три-діагональні, або п'яти-діагональні, або $l$-діагональні (eng. \textit{banded}), то і $F$ також, причому якщо $F$ --- $k$-діагональна, то факторизацію можна провести а $O\left(nk^2\right)$, а зворотній хід --- за $O(nk)$. Сумарна вартість кроку оновлення змінної $x$ у цьому випадку складає $O\left(nk^2\right)$.
\end{example}

\begin{remark}
    Схожий підхід працює якщо $P + \rho A^\intercal A$ має більш загальний патерн розрідженості, тоді використовують факторизацію Холецького з перестановкою.
\end{remark}

\subsubsection{Кешування факторизації}

Припустимо, що тепер нам необхідно розв'язати багато систем вигляду $F x^{(i)} = g^{(i)}$, де $i = 1, \ldots, N$, з однаковою матрицею коефіцієнтів, але різними правими частинами. Така ситуація виникає коли штрафний параметр $\rho$ не змінюється в ADMM (принаймні протягом кількох  ітерацій). Зрозуміло, що у цьому випадку факторизацію можна виконати лише один раз, а потім просто виконувати кілька послідовних зворотних ходів для кожної правої частини. \medskip

Якщо обчислювальна вартість факторизації була $t$, а обчислювальна складність зворотного ходу $s$, то сумарна вартість $N$ ітерацій стає $t + N \cdot s$ замість $N \cdot (t + s)$ яку ми б отримали якби проводили факторизацію $F$ на кожному кроці. Тому поки $\rho$ не змінюється, ми можемо  розкладати на множники $P + \rho A^\intercal A$ один раз і потім використовувати цю факторизацію у послідовних кроках оновлення змінної $x$. 

\begin{example}
    Якщо ми навіть не експлуатували розрідженість і використовували стандартну факторизацію Холецького, то такий підхід дозволяє виконувати кроки оновлення змінної $x$ асимптотично (при кількості цих кроків що прямує до нескінченності) ефективніше ніж при наївній реалізації у $n$ разів!
\end{example}

\begin{remark}
    Коли розрідженість експлуатується, то відношення $t:s$ зазвичай менше за $n$ але все ще відчутне, тому тут також є виграш у ефективності. Щоправда, у цьому випадку виграш від відсутності необхідності перераховувати факторизацію $P + \rho A^\intercal A$ (тобто незмінності $\rho$) менший, що змушує зважувати його супроти виграшу від зміни $\rho$, яка як ми бачили в \S3.4.1 може суттєво покращувати швидкість збіжності.
\end{remark}

Резюмуючи, ефективна реалізація завжди має запам'ятовувати факторизацію $P + \rho A^\intercal A$ і пере-обчислювати її тільки при зміні $\rho$, а яким саме буде виграш у ефективності --- залежить від конкретної задачі, але він точно є.

\subsubsection{Лема про обернення матриці}

\begin{lemma}[про обернення матриці]
    Рівність нижче виконується якщо всі обернені існують
    
    \begin{equation}
    	\left(P + \rho A^\intercal A\right)^{-1} = P^{-1} - \rho P^{-1} A^\intercal \cdot 
    	\left( I + \rho A P^{-1} A^\intercal \right)^{-1} \cdot A P^{-1}.
    \end{equation}
\end{lemma}

Це означає, що СЛАР з матрицею коефіцієнтів $P$ можна ефективно розв'язати і $p$ на порядок менше за $n$ (або принаймні не більше), то і крок оновлення змінної $x$ можна виконати ефективно. Для цього використовується той же трюк: кешується факторизація $I + \rho A P^{-1} A^\intercal$ і  використовуються дешевші зворотні ходи.

\begin{example}
    Якщо $P$ діагональна то наївна реалізація потребує $O\left(n^3\right)$ флопс на першій ітерації і по $O\left(n^2\right)$ на кожній наступній. Якщо тепер $p \le n$, то збереження множників правої частини рівності з леми замість множників $P + \rho A^\intercal A$  дозволить провести факторизацію за $O\left(np^2\right)$ флопс, у $(n / p)^2$ разів ефективніше, а потім ще й проводити зворотні ходи за $O(np)$ флопс, що у $n / p$ разів ефективніше.
\end{example}

\begin{remark}
    Використання леми про обернення матриці для обчислення $x^+$ також робить дешевшою зміну $\rho$ від ітерації до ітерації: коли $P$ діагональна то ми можемо обчислити $AP^{-1}T$ один раз і потім створювати $I + \rho^{k} A P^{-1} A^\intercal$ на ітерації $k$ за $O\left(p^3\right)$ флопс. Іншими словами, оновлення $\rho$ коштує додаткових $O(np)$ флопс, тому коли $p^2$ менше або дорівнює $n$ (за порядком), то немає додаткової складності (за порядком) у тому аби оновлювати $\rho$ хоч на кожній ітерації.
\end{remark}

\subsubsection{Квадратична функція на афінній множині}

Ті ж зауваження справедливі для трохи складнішого випадку опуклої квадратичної функції що розглядається на афінній множині:

\begin{equation}
	f(x) = (1 / 2) \cdot x^\intercal P x + q^\intercal x + r, \quad \domain f = \{ x | F x = g \}.
\end{equation}

У цьому випадку $x^+$ все ще є афінною функцією змінної $v$, а крок оновлення змінної $x$ потребує розв'язання системи KKT (Каруша-Куна-Такера):

\begin{equation}
	\begin{pmatrix} P + \rho I & F^\intercal \\ F & \textbf{0} \end{pmatrix} \cdot
	\begin{pmatrix} x^{k + 1} \\ v \end{pmatrix} +
	\begin{pmatrix} q - \rho \cdot (z^k - u^k) \\ -g \end{pmatrix} = \textbf{0}.
\end{equation}

Бачимо, що всі попередні зауваження справедливі і тут: факторизації все ще можна кешувати, а структура матриць все ще може бути експлуатованою для покращення ефективності факторизації і зворотнього ходу.

\subsection{Гладкі цільові доданки}

\subsubsection{Ітеративні методи}

Коли $f$ гладка, то для виконання мінімізації по $x$ можуть бути застосовані загальні ітеративні методи. Найцікавішим є клас методів які вимагають тільки вміння рахувати $\nabla f(x)$ для заданого $x$, множити вектор на матрицю $A$, і множини вектор на матрицю $A^\intercal$. Справа в тому, що саме такі методи гарно підходять для задач великого масштабу.

\begin{example}
    До таких алгоритмів належать стандартний градієнтний метод, (нелінійний) спряжений градієнтний метод, а також алгоритм Бройдена-Флетчера-Гольдфарба-Шенно з обмеженою пам'яттю, більш відомий як L-BFGS (див. \cite{113, 26}).
\end{example}

Також див. \cite{135} для подальших подробиць. \medskip

Збіжність цих методів залежить від обумовленості функції що мінімізується. 

\begin{remark}
    Наявність квадратичного штрафного доданку $(\rho / 2) \cdot \|A x - v\|_2^2$ на практиці покращує обумовленість задачі, а тому і пришвидшує збіжність ітеративних методів.
\end{remark}

\begin{remark}
    Одним з методів зміни параметру $\rho$ від ітерації до ітерації полягає у тому, щоб збільшувати його поки відповідний ітеративний алгоритм не почне збігатися достатньо швидко.
\end{remark}

\subsubsection{Рання зупинка}

Стандартною технікою що пришвидшує алгоритм є зупиняти ітеративний метод що виконує оновлення змінної $x$ є не доводити його до кінця, тобто перед тим як градієнт від $f(x) + (\rho / 2) \cdot \|A x - v\|_2^2$ стає дуже малим. Ця техніка виправдовується результатами щодо збіжності ADMM з неточною  мінімізацією на кроках оновлення змінних $x$ та $z$. \medskip

У цій техніці, на перших кроках оновлення можна зупинятися при досягненні хоча б якоїсь низької точності яка має підвищуватися ближче по мірі кроків щоб гарантувати збіжність. Зрозуміло, що такий підхід призводить до потенційно більшої кількості кроків, зате самі кроки стають набагато дешевшими з обчислювальної точки зору бо потребують значно менше ітерацій ітеративного методу.

\subsubsection{Теплий старт}

Іншим стандартним трюком є ініціалізація ітеративного методу що використовується у кроці оновлення змінної $x$ не у випадковій чи нульовій точці, а на результаті на якому відбувалася зупинка ітеративного методу на попередньому кроці. \medskip

Ця техніка називається \textit{теплим стартом}, бо ми ніби граємо у ``тепло-холодно'' і починаємо одразу з ``тепло'', бо зачасту результат попереднього кроку же дає дуже гарне наближення для розв'язку поточного кроку, що дозволяє ще більше зменшити кількість ітерацій ітеративного методу на один крок оновлення змінної $x$. Особливо відчутна перевага на останніх кроках, коли ADMM вже майже збігся і $x^{k + 1}$ не може сильно відрізнятися від $x^k$.

\subsubsection{Квадратичні цільові доданки}

Навіть якщо $f$ квадратична, може виявитися що краще застосовувати ітеративний а не аналітичний алгоритм для оновлення змінної $x$. У таких випадках можна  використовувати стандартний (можливо передумовлений) метод спряженого градієнту.
\medskip

Цей підхід э розумним якщо аналітичні методи не працюють, наприклад коли вони потребують надто багато пам'яті, або коли $A$ густа, але відомий швидкий метод множення векторів на $A$ або на $A^\intercal$.

\begin{example}
    Одним з частих таких випадків є матриці $A$ що представляють дискретне перетворення Фур'є, див. \cite{90}.
\end{example}

\subsection{Декомпозиція}

\subsubsection{Блочна ``роздільність''}

Нехай $x = \begin{pmatrix} x_1 & x_2 & \cdots & x_N \end{pmatrix}^\intercal$ --- певне розбиття вектор-змінної $x$ на під-вектори таке, що $f$ ``роздільна'' відносно цього розбиття, тобто 

\begin{equation}
    f(x) = f_1(x_1) + f_2(x_2) + \ldots + f_n(x_n), 
\end{equation}

де $x_i \in \RR^{n_i}$ і $\sum_{i = 1}^N n_i = N$. Якщо квадратичний доданок $\|A x\|_2^2$ також роздільний відносно цього розбиття, тобто якщо $A^\intercal \cdot A$ блочно-діагональна матриця відносно цього розбиття, то і доповнена функція Лагранжа $L_\rho$ також роздільна. \medskip

Це все разом означає, що крок оновлення змінної $x$ може бути обчисленим паралельно, де кожний під-вектор $x_i$ бере участь в своїй власній окремій від інших мінімізації.

\subsubsection{По-компонентна роздільність}

У деяких випадках декомпозиція можлива аж до рівня окремих компонент, тобто 

\begin{equation}
    f(x) = f_1(x_1) + \ldots + f_n(x_n), 
\end{equation}

де $f_i: \RR \to \RR$, а  $A^\intercal \cdot A$ --- діагональна матриця. Тоді крок мінімізації за змінною $x$ може бути виконаним як $n$ мінімізацій за \textit{скалярними} змінними, які інколи можуть бути виконаними аналітично, але у будь-якому разі відбуваються надзвичайно ефективно. Така ситуація називається \textit{по-компонентною роздільністю}.

\subsubsection{М'яке порогування}

\begin{example}
    На практиці часто зустрічається функція $f(x) = \lambda \cdot \|x\|_1$, де $\lambda > 0$ і $A = I$. У цьому випадку (скалярні) оновлення змінних $x_i$ набувають вигляду
    
    \begin{equation}
        x_i^+ \coloneqq \argmin_{x_i}  \left( \lambda \cdot |x_i| + (\rho / 2) \cdot (x_i - v_i)^2 \right).
    \end{equation}
\end{example}

Не зважаючи на недиференційовність першого доданку ми все ще запросто можемо знайти розв'язок цієї задачі у замкненій формі використовуючи субдиференціальне числення (див.~\cite[\S23]{140} для бекграунду). А саме, розв'язок набуває вигляду

\begin{equation}
    x_i^+ \coloneqq S_{\lambda/\rho}(v_i), 
\end{equation}

де $S$ --- \textit{оператор м'якого порогування}, визначений наступним чином:

\begin{equation}
    S_\kappa(a) = \begin{cases} 
        a - \kappa, & a > \kappa, \\ 
        0, & |a| \le \kappa, \\ 
        a + \kappa, & a < - \kappa, 
    \end{cases}
\end{equation}

або, що те саме,

\begin{equation}
    S_\kappa(a) = (a - \kappa)_+ - (- a - \kappa)_+.
\end{equation}

Ось ще одна формула яка явно показує, що оператор м'якого порогування є оператором \textit{стиску} у тому розумінні, що він рухає точки до нуля, тобто зменшує $\|\cdot\|_1$-норму:

\begin{equation}
    S_\kappa(a) = (1 - \kappa/|a|)_+ \cdot a,
\end{equation}

для $a \ne 0$. Оновлення компонент що  виражаються у такому вигляді називатимемо по-компонентним м'яким порогуванням. На мові \S4.1, оператор м'якого порогування є проксимальним оператором для норми $\ell_1$.
