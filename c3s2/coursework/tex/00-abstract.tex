Багато актуальних задач статистики та машинного навчання можуть бути сформульовані у термінах опуклої оптимізації. Сьогодні розміри та складність даних зростають з експоненційною швидкістю, і разом із цим стає все більш важливим вміти розв'язувати задачі із великою кількістю ознак у об'єктів та/або великою кількістю самих об'єктів. Як наслідок, як децентралізований збір та збереження даних, так і розподілені методи розв'язування задач є необхідними, або принаймні дуже бажаними. У цьому огляді ми намагаємося показати, що метод множників які змінюють напрямок гарно підходить для розподіленої опуклої оптимізації, і зокрема до задач великого масштабу, що виникають у статистиці, машинному навчанні, та суміжних областях. Метод був розроблений у 1970-их, з коренями у 1950-их, і є еквівалентним, або тісно пов'язаним з багатьма іншими алгоритмами, такими як двоїстий розклад, метод множників (Лагранжа), розбиття Дугласа-Рашфорда, метод часткових обернених Спінгарна, метод почергових проекцій Дейкстри, ітеративними алгоритмами Брегмана для задач з нормою $\ell_1$, проксимальними (eng. \textit{proximal}) методами, та іншими. Після короткого огляду теоретичних результатів та історії алгоритму, ми обговоримо його застосування до широкого кола актуальних задач статистики та машинного навчання, включаючи ласо (eng. \textit{lasso}), розріджену логістичну регресію, вибір базису (eng. \textit{basis pursuit}), оцінку матриці коваріації (eng. \textit{covariance selection}), машини опорних векторів (eng. \textit{support vector machine}), та багато інших. 

% Ми також обговоримо загальну розподілену оптимізацію, розширення до неопуклої постановки, та ефективну реалізацію.