Задачі що розглядаються у цьому розділі допоможуть продемонструвати, чому ADMM є природнім вибором для задач машинного навчання і статистики. Причина у тому, що на відміну від двоїстого сходження чи методу множників, ADMM явно націлюються на задачі які розділяються на дві різні частини, $f$ і $g$, з якими можна розбиратися незалежно. \medskip

Задачі такого вигляду домінують у машинному навчанні, адже значна кількість задач навчання передбачає мінімізацію функції втрат разом із штрафнии доданком, або із сторонніми обмеженнями. У інших випадках ці обмеження виникають за рахунок переведення початкової задачі у якусь загальноприйнятий вигляд.  \medskip
% Зокрема, задачі консенсусу будуть розглянуті у \S7.1.

Цей розділ містить багато простих але важливих задач з $\ell_1$ нормами. На сьогоднішній день зацікавленість у цих задачах особливо поширена серед спеціалістів по задачам статистики, машинного навчання, і обробки сигналів. \medskip

Застосування ADMM зачасту призводить до state-of-the-art методів, або до близьких до таких. Ми покажемо, що ADMM дозволяє природнім чином виділити негладкий $\ell_1$ доданок з гладкої функції втрат, що є вигідним з обчислювальної точки зору. У цьому розділі ми сфокусуємося на не розподілених версіях цих задач для простоти; задачі розподіленого підбору моделей будуть розглянуті далі. \medskip

Ідеї $\ell_1$-регуляризації вже десятки років, вона простежується з роботи Губера \cite{100} щодо стійкої статистики, і статті Клербо \cite{38} з геофізики. Існує обширна література з цього приводу, але кількома важливими сучасними статтями є \cite{145} щодо варіаційного знешумлення, \cite{49} щодо м'якого порогування, \cite{156} щодо ласо, \cite{34} щодо виділення базису, \cite{50, 28, 29} щодо скомпресованого відчуття і \cite{123} щодо структурного навчання розріджених графічних моделей. \medskip

Поширення моделей з $\ell_1$-регуляризацією викликало значний обсяг досліджень алгоритмів розв'язування таких задач. Недавній огляд Янга та ін. \cite{173} порівнює швидкодію багатьох різних алгоритмів, включаючи метод проекції градієнту \cite{73, 102}, гомотопічні методи \cite{52}, ітеративне звуження порогування \cite{45}, проксимальні градієнтні методи \cite{132, 133, 11, 12}, доповнені методи Лагранжа \cite{175}, і методи внутрішньої точки \cite{103}. \medskip

Є також інші підходи, як-то ітеративні алгоритми Брегмана \cite{176} і ітеративні алгоритми порогування \cite{51}, які реалізуються у фреймворку обміну повідомленнями.

\subsection{Найменше абсолютне відхилення}

Спрощеним варіантом задачі найменших квадратів є \textit{найменше абсолютне відхилення}, у якому ми мінімізуємо $\|A x - b\|_1$ замість $\|A x - b\|_2$. Найменші абсолютні відхилення надають більш стійку модель ніж МНК у випадках коли дані містять великі аномалії, і часто використовуються у статистиці та економетриці\footnote{Див., наприклад, \cite[\S10.6]{95}, \cite[\S9.6]{171}, і \cite[\S6.1.2]{20}.}. \medskip

У формі ADMM цю задачу можна записати як 

\begin{equation}
    \|z\|_1 \xrightarrow[A x - z = b]{} \min,
\end{equation}

тобто $f = 0$ і $g = \|\cdot\|_1$. \medskip

Використання особливого вигляду $f$ і $g$ у припущенні невиродженості $A^\intercal A$ дозволяє записати ітерації у наступному вигляді:

\begin{align}
    x^{k + 1} &\coloneqq \left(A^\intercal A\right)^{-1} A^\intercal \left(b + z^k - u^k\right) \\
    z^{k + 1} &\coloneqq S_{1 / \rho} \left( A x^{k + 1} - b + u^k \right) \\
    u^{k + 1} &\coloneqq u^k + A x^{k + 1} - z^{k + 1} - b,
\end{align}

де оператор м'якого порогування застосовується по-координатно. 

\begin{remark}
    Як і в \S4.2, матрицю $A^\intercal A$ можна факторизувати всього лише один раз, а закешовані множники потім використовуються для здешевлення подальших кроків оновлення змінної $x$.
\end{remark}

Крок оновлення змінної $x$ це МНК із матрицею коефіцієнтів $A$ і правою частиною $b + z^k - u^k$. Таким чином ADMM можна розглядати як метод розв'язування задачі мінімальних абсолютних відхилень шляхом ітеративного розв'язування асоційованих задач найменших квадратів зі змінною правою частиною. Змінена права частина оновлюється оператором м'якого порогування.

\begin{remark}
    З кешуванням факторизації обчислювальна складність розв'язування подальших задач найменших квадратів набагато менше ніж першої, що зачасту дозволяє розв'язувати задачу найменших абсолютних відхилень ледь не так само швидко як і задачу найменших квадратів.
\end{remark}

\subsubsection{Задача Губера}

Задачею, яка знаходиться між найменшими квадратами і найменшими абсолютними відхиленнями є задача з \textit{функцією Губера}:

\begin{equation}
    g^{\text{hub}} (A x - b) \to \min,
\end{equation}

де \textit{штрафна функція Губера} є квадратичною для малих значень аргументу і переходить у абсолютне значення для більших значень аргументів. 

\begin{definition}
    Для скаляру $a$ вона задається наступним чином:

    \begin{equation}
        g^{\text{hub}}(a) = \begin{cases}
            a^2 / 2, & |a| \le 1, \\
            |a| - 1 / 2, & |a| > 1.
        \end{cases}
    \end{equation}
\end{definition}

\begin{remark}
    На векторний аргумент функція Губера узагальнюється як сума значень функції Губера на компонентах.
\end{remark}

\begin{remark}
    Для простоти ми розглядаємо стандартизовану функцію Губера яка змінює поведінку з квадратичної на лінійну у точках з $|a| = 1$, хоча взагалі кажучи можна розглядати $|a| = \text{const}$.
\end{remark}

Цю задачу можна розглядати у такій же ADMM формі як і вище, з єдиною різницею у кроці оновлення змінної $z$. Тепер він використовуватиме проксимальний оператор функції Губера а не $\ell_1$-норми:

\begin{equation}
    z^{k + 1} \coloneqq \frac{\rho}{1 + \rho} \left( A x^{k + 1} - b + u^k \right) + \frac{1}{1 + \rho} \cdot S_{1 + 1/\rho} \left( A x^{k + 1} - b + u^k \right).
\end{equation}

\begin{remark}
    Коли розв'язок МНК $x^{\text{ls}} = \left( A^\intercal A \right)^{-1} b$ задовольняє умову $\left| x_i^{\text{ls}} \right| \le 1$ для всіх $i$, то він також є розв'язком задачі Губера, і ADMM зупиняється за два кроки.
\end{remark}

\subsection{Вибір базису}

\textit{Вибір базису} це задача мінімізації з нормою $\ell_1$ з обмеженням типу рівність у формі

\begin{equation}
    \|x\|_1 \xrightarrow[A x = b]{} \min,
\end{equation}

де змінна $x \in \RR^n$, $A \in \RR^{m \times n}$, $b \in \RR^m$, $m < n$. \medskip

Вибір базису часто застосовується як евристика для пошуку розрідженого розв'язку недовизначеної СЛАР. 

\begin{remark}
    Ця задача відіграє центральну роль у сучасній статистичній обробці сигналів, зокрема у теорії стиснутого сприйняття (див.~\cite{24} для свіжого огляду стану справ у цій галузі).
\end{remark}

У ADMM-формі задача вибору базису може бути записана так:

\begin{equation}
    f(x) + \|z\|_1 \xrightarrow[x - z = 0]{} \min,
\end{equation}

де $f$ --- індикаторна функція множини $\{x \in \RR^n \mid A x = b\}$. \medskip

Тоді ітерації ADMM запишуться у вигляді

\begin{align}
    x^{k + 1} &\coloneqq \Pi \left(z^k - u^k\right) \\
    z^{k + 1} &\coloneqq S_{1 / \rho} \left(x^{k + 1} + u^k\right) \\
    u^{k + 1} &\coloneqq u^k + x^{k + 1} - z^{k + 1},
\end{align}

де $\Pi$ --- оператор проектування на $\{x \in \RR^n \mid A x = b\}$. \medskip

Крок оновлення змінної $x$ який вимагає розв'язування задачі мінімізації Евклідової норми з лінійним обмеженням можна явно записати наступним чином:

\begin{equation}
    x^{k + 1} \coloneqq \left( I - A^\intercal (A A^\intercal)^{-1} A \right) \left(z^k - u^k\right) + A^\intercal (A A^\intercal)^{-1} b.
\end{equation}

\begin{remark}
    Знову ж таки, зауваження з \S4.2 щодо ефективної реалізації застосовні тут; кешування факторизації $A A^\intercal$ робить подальші проекції значно обчислювально дешевшими ніж першу.
\end{remark}

\begin{remark}
    Можна інтерпретувати ADMM для задачі вибору базису як зведення однієї задачі мінімізації $\ell_1$ норми до розв'язування послідовності задач мінімізації Евклідової норми. Для обговорення схожих алгоритмів для обробки зображень див.~\cite{2}.
\end{remark}

\begin{remark}
    Не так давно виник ще один клас алгоритмів, так звані ітеративні методи Брегмана, які подають надії для задач з нормою $\ell_1$. Для задачі вибору базису і суміжних, ітеративна регуляризація Брегмана \cite{176} виявляється еквівалентною методу множників, а розділений метод Брегмана \cite{88} еквівалентний до ADMM (див.~\cite{68} для доведення).
\end{remark}

\subsection{Загальна задача мінімізації з \texorpdfstring{$\ell_1$}{l1}-ре\-гу\-ля\-ри\-за\-ці\-є\-ю}

Розглянемо загальну задачу

\begin{equation}
    \label{eq:generic-ell-1}
    \ell(x) + \lambda \|x\|_1 \to \min,
\end{equation}

де $\ell$ --- якась опукла функція втрат. \medskip

У ADMM-формі цю задачу можна записати як

\begin{equation}
    \ell(x) + g(z) \xrightarrow[x - z = 0]{} \min,
\end{equation}

де $g(z) = \lambda \|z\|_1$. \medskip

Тоді ітерації будуть

\begin{align}
    x^{k + 1} &\coloneqq \Argmin_x \left( \ell(x) + (\rho / 2) \left\| x - z^k + u^k \right\|_2^2 \right) \\
    z^{k + 1} &\coloneqq s_{\lambda / \rho} \left( x^{k + 1} + u^k \right) \\
    u^{k + 1} &\coloneqq u^k + x^{k + 1} - z^{k + 1}.
\end{align}

\begin{remark}
    Крок оновлення змінної $x$ вимагає обчислення проксимального оператору. 
\end{remark}

\begin{example}
    Якщо $\ell$ гладка, то це можна зробити довільним стандартним методом, як-то метод Ньютона, або квазі-ньютонівським методом, як-то L-BFGS, або методом спряженого градієнту.
\end{example}

\begin{example}
    Якщо $\ell$ квадратична, то крок мінімізації змінної $x$ можна виконати розв'язавши СЛАР, як описано в \S4.2.
\end{example}

\begin{remark}
    У загальному можна інтерпретувати ADMM для функції втрат з $\ell_1$-регуляризацією як спрощення мінімізації цієї функції до послідовності мінімізацій задач з функціями втрат з $\ell_2$.
\end{remark}

\begin{remark}
    Дуже велика кількість моделей може бути представлена у подібному вигляді, включаючи узагальнені лінійні моделі \cite{122} і узагальнені адитивні моделі \cite{94}.
\end{remark}

\begin{example}
    Узагальнені лінійні моделі включають лінійну регресію, логістичну регресію, softmax регресію, і навіть регресію Пуассона, адже вони дозволяють довільну експоненційну сім'ю функцій. 
\end{example}

\begin{remark}
    Для загального опису моделей типу логістичної регресії з $\ell_1$-ре\-гу\-ля\-ри\-за\-ці\-є\-ю див., наприклад, \cite[\S4.4.4]{95}.
\end{remark}

\begin{remark}
    Для використання будь-якого іншого регуляризатора $g(z)$ замість $\|z\|_1$, ми просто замінюємо оператор м'якого порогування у кроці оновлення змінної $z$ на проксимальний оператор функції $g$, як описано в \S4.1.
\end{remark}

\subsection{Ласо}

Важливим частинним випадком задачі \eqref{eq:generic-ell-1} є лінійна регресія з $\ell_1$-регуляризацією, яка також називається ласо \cite{156}. Вона передбачає розв'язання задачі

\begin{equation}
    (1/2) \left\| A x - b \right\|_2^2 + \lambda \|x\|_1,
\end{equation}

де $\lambda > 0$ --- скалярний параметр регуляризації, який зазвичай обирається шляхом перехресної валідації. \medskip

У класичних застосуваннях зачасту є набагато більше факторів ніж тренувальних об'єктів, а ціллю є побудова якомога простішої моделі.

\begin{remark}
    Для загального огляду ласо див.~\cite[\S3.4.2]{95}.
\end{remark}

Ласо часто застосовується, зокрема для аналізу біологічних даних, де лише мала частина з дуже великої кількості потенційних факторів реально впливає на цікавий для дослідників результат.

\begin{remark}
    Для детального огляду подібних медичних досліджень див.~\cite[\S18.4]{95}.
\end{remark}

У ADMM-формі цю задачу можна записати наступним чином:

\begin{equation}
    f(x) + g(z) \xrightarrow[x - z = 0]{} \min,
\end{equation}

де $f(x) = (1 / 2) \|A x - b\|_2^2$, а $g(z) = \lambda \|z\|_1$. \medskip

Використовуючи зауваження з \S4.2 і \S4.4, можемо записати наступні ітерації:

\begin{align}
    x^{k + 1} &\coloneqq \left( A^\intercal A + \rho I \right)^{-1} \left( A^\intercal b + \rho \left( z^k - u^k \right) \right) \\
    z^{k + 1} &\coloneqq S_{\lambda / \rho} \left( x^{k + 1} + u^k \right) \\
    u^{k + 1} &\coloneqq u^k + x^{k + 1} - z^{K + 1}.
\end{align}

\begin{remark}
    Матриця $A^\intercal A + \rho I$ завжди невироджена, бо $\rho > 0$.
\end{remark}

\begin{remark}
    Крок оновлення змінної $x$ є по суті гребеневою регресією (тобто квадратично регуляризованим МНК), тому ADMM можна інтерпретувати як метод розв'язування задачі ласо шляхом розв'язування послідовності гребеневих регресій.
\end{remark}

\begin{remark}
    При використанні прямих методів роз\-в'яз\-у\-ван\-ня задачі гребеневої регресії обчислювально вигідно кешувати факторизацію на першій ітерації, аби подальші можна було виконати з меншими обчислювальними складностями.
\end{remark}

\begin{remark}
    Див.~\cite{1} для прикладу застосування в обробці зображень.
\end{remark}

\subsubsection{Узагальнене ласо}

Задачу ласо можна узагальнити наступним чином:

\begin{equation}
    (1/2) \left\|A x - b\right\|_2^2 + \lambda \left\| F x \right\|_1 \to \min,
\end{equation}

де $F$ --- довільне лінійне перетворення. \medskip

\begin{example}
    Важливим випадком є різницева матриця $F \in \RR^{(n - 1) \times n}$, тобто

    \begin{equation}
        F_{i,j} = \begin{cases}
            1, & j = i + 1, \\
            -1, & j = i, \\
            0, & \text{інакше},
        \end{cases}
    \end{equation}

    і одинична матриця $A = I$, тоді отримуємо наступну задачу:

    \begin{equation}
        (1 / 2) \| x - b \|_2^2 + \lambda \Sum_{i = 1}^{n - 1} |x_{i + 1} - x_i|.
    \end{equation}
\end{example}

\begin{definition}
    Тут другий доданок є \textit{сумарною варіацією} від $x$. Ця задача зазвичай називається \textit{знешумленням сумарної варіації} \cite{145} і має багато застосувань у обробці сигналів. 
\end{definition}

\begin{definition}
    Коли $A = I$, а $F$ є другою різницевою матрицею, то узагальнена задача ласо має назву $\ell_1$-\textit{підбору тренду} \cite{101}.
\end{definition}

У ADMM-формі узагальнену задачу ласо можна записати наступним чином:

\begin{equation}
    (1/2) \left\| A x - b \right\|_2^2 + \lambda \|z\|_1 \xrightarrow[F x - z = 0]{} \min,
\end{equation}

що призводить до наступних ітерацій:

\begin{align}
    x^{k + 1} &\coloneqq \left( A^\intercal A + \rho F^\intercal F \right)^{-1} \left( A^\intercal b + \rho F^\intercal \left( z^k - u^k \right) \right) \\
    z^{k + 1} &\coloneqq S_{\lambda / \rho} \left( F x^{k + 1} + u^k \right) \\
    u^{k + 1} &\coloneqq u^k + F x^{k + 1} - z^{k + 1}.
\end{align}

\begin{example}
    Для випадку знешумлення сумарної варіації матриця $A^\intercal A + \rho F^\intercal F$ є тридіагональною, тому крок оновлення змінної $x$ можна виконати за $O(n)$ флопс (див.~\cite[\S4.3]{90}).
\end{example}

\begin{example}
    Для $\ell_1$-підбору тренду ця матриця є п'ятидіагональною, і крок оновлення змінної $x$ все ще може бути виконаним за $O(n)$ флопс.
\end{example}

\subsubsection{Групове (блочне) ласо}

Іншим прикладом є заміна регуляризатора $\|x\|_1$ на 

\begin{equation}
    \Sum_{I = 1}^N \|x_i\|_2,
\end{equation}

де $x = \begin{pmatrix} x_1 & \cdots & x_N \end{pmatrix}$, де $x_i \in \RR^{n_i}$. Зрозуміло, що якщо $n_i = 1$ і $N = n$, то це просто ласо. \medskip

Як бачимо, тут регуляризатор є розділимим відносно розбиття $x_1, \ldots, x_N$, але не цілком розділимим. Це розширення $\ell_1$-резуляризації називається \textit{групове ласо} \cite{177}, або, більш загально, \textit{регуляризація сумою норм} \cite{136}. \medskip

ADMM для цієї задачі такий же як і вище, за виключенням того, що крок оновлення змінної $z$ може бути заміненим наступним блочним м'яким порогуванням:

\begin{equation}
    z_i^{k + 1} = S_{\lambda / \rho} \left( x_i^{k + 1} + u^k \right), \quad i = 1, \ldots, N,
\end{equation}

де оператор м'якого порогування $S_\kappa: \RR^m \to \RR^m$ має вигляд

\begin{equation}
    S_\kappa (a) = \left(1 - \kappa / \|a\|_2 \right)_+ a,
\end{equation}

де $S_\kappa(0) = 0$. Ця формула є звичайним скалярним оператором м'якого порогування коли $a$ --- скаляр, і узагальнює вигляд даний в \S4.2. \medskip

Подібний підхід можна розвинути щоб працювати з недиз'юнктним розбиттям, що часто зустрічається у біоінформатиці та інших прикладних випадках \cite{181, 118}. А саме, якщо є $N$ потенційно перетинних груп $G_i \subseteq \{1, \ldots, n\}$, а цільова функція має вигляд

\begin{equation}
    (1/2) \left\| A x - b\right\|_2^2 + \lambda \Sum_{i = 1}^N \left\| x_{G_i} \right\|_2,
\end{equation}

де $x_{G_i}$ --- підвектор, що містить компоненти з індексами з $G_i$. Оскільки групи можуть перетинатися, то цю задачу дуже складно розв'язати більшістю класичних методів, але виявляється що вона робиться ``в лоб'' за допомогою ADMM. \medskip

Справді, введемо $N$ нових змінних $x_i \in \RR^{|G_i|}$, і розглянемо задачу

\begin{equation}
    (1/2) \left\| A z - b\right\|_2^2 + \lambda \Sum_{i = 1}^N \left\| x_i \right\|_2 \xrightarrow[\substack{x_i - z_i = 0, \\ i = 1, \ldots, N}]{} \min,
\end{equation}

де змінні $x_i$ локальні, змінна $z$ глобальна, а $z_i$ --- ``уявлення'' $z$ про те, якими мають бути $x_i$, воно визначається лінійною функцією від $z$. 
% Подібна термінологія відповідає оптимізаційній задачі консенсусу, яка детально описана у \S7.2, і частинним випадком якої є групове ласо із перетинними групами.

\subsection{Оцінка матриці коваріації з розрідженою оберненою}

Нехай задано вибірку значень з нормального розподілу в $\RR^n$:

\begin{equation}
    a_i \sim \mathcal{N}(0, \Sigma), \quad i = 1, \ldots, N.
\end{equation}

Розглянемо задачу оцінки матриці коваріації $\Sigma$ у припущенні що $\Sigma^{-1}$ розріджена. Оскільки $\left(\Sigma^{-1}\right)_{i,j}$ дорівнює нулю тоді і тільки тоді, коли $i$-та і $j$-та компоненти випадкової величини умовно незалежні, то ця задача є еквівалентною задачі \textit{структурного навчання} оцінки топології неорієнтованої графічної моделі представлення Гаусіану \cite{104}. 

\begin{definition}
    Визначення природи розрідженості оберненої матриці коваріації $\Sigma^{-1}$ також називається задачею \textit{вибору коваріації}.
\end{definition}

\begin{remark}
    Для дуже малих $n$ теоретично можливо перебрати усі можливі патерни розрідженості в $\Sigma^{-1}$, оскільки для фіксованого патерну задача максимізації вірогідності $\Sigma$ є задачею опуклої оптимізації.
\end{remark}

Гарною евристикою яка поширюється на набагато більші значення $n$ є мінімізація мінус логарифму вірогідності (по параметру $X = \Sigma^{-1}$) з $\ell_1$-регуляризацією для ``підтримки'' розрідженішим варіантам \cite{7}. Якщо $S$ --- емпірична матриця коваріації, тобто

\begin{equation}
    S = \frac{1}{N} \Sum_{i = 1}^N a_i a_i^\intercal,
\end{equation}

то нашу задачу (мінімізації мінус логарифму вірогідності) можна записати у вигляді

\begin{equation}
    \trace (S X) - \log \det X + \lambda \|X\|_1 \to \min,
\end{equation}

де змінна $X \in S_+^n$, а $\|\cdot\|_1$ визначена поелементно, тобто як сума модулів усіх елементів матриці, а областю визначення $\log \det$ є $S_{++}^n$, множина симетричних додатновизначених $n\times n$ матриць. Це особливий випадок загальної $\ell_1$-регуляризованої задачі з опуклою функцією втрат

\begin{equation}
    \ell(X) = \trace (S X) - \log \det X.
\end{equation}

Ідея вибору коваріації належить Демпстеру \cite{48} і вперше була розглянути у розрідженому високорозмірному випадку Майншаусеном і Бюльманом \cite{123}. Вищенаведена постановка задачі належить Банджері та ін. \cite{7} Деякі недавні статті щодо цієї задачі включають \textit{графічне ласо} Фрідмена та ін.~\cite{79}, роботу Дачу та ін. \cite{55}, Лу \cite{115}, Юана \cite{178} і Шайнберга та ін.~\cite{148}, яка показу, що ADMM перевершує state-of-the-art алгоритми для цієї задачі. \medskip

Ітерації ADMM мають вигляд

\begin{align}
    X^{k + 1} &\coloneqq \Argmin_X \left( \trace (SX) - \log \det X + (\rho / 2) \left\|X - Z^K + U^k \right\|_F^2\right) \\
    Z^{k + 1} &\coloneqq \Argmin_Z \left( \lambda\|Z\|_1 + (\rho/2)\left\|X^{k + 1} - Z + U^k\right\|_F^2\right) \\
    U^{k + 1} &\coloneqq U^k + X^{K + 1} - Z^{k + 1}
\end{align}

де $\|\cdot\|_F$ --- норма Фробеніуса, тобто квадратний корінь з суми квадратів елементів.

\begin{proposition}
    Цей алгоритм можна значно спростити.
\end{proposition}

\begin{proof}
    Крок оновлення змінної $Z$ є поелементним м'яким порогуванням:

    \begin{equation}
        Z_{i,j}^{k + 1} \coloneqq S_{\lambda/\rho} \left( X_{i,j}^{K + 1} + U_{i, j}^k \right),
    \end{equation}
    
    а крок оновлення змінної $X$, як виявляється, має має аналітичний розв'язок. 
    
    Справді, критерієм  оптимальності першого порядку є рівність градієнта нулеві, тобто
    
    \begin{equation}
        S - X^{-1} + \rho \left( X - z^k + U^k \right) = 0,
    \end{equation}
    
    разом з неявним обмеженням $X \succ 0$. Це можна переписати як
    
    \begin{equation}
        \label{eq:6.5}
        \rho X - X^{-1} = \rho \left( Z^k - U^k \right) - S.
    \end{equation}
    
    Побудуємо тепер матрицю $X$ яка задовольняє цій умові, а тому і мінімізує цільову функцію у кроці оновлення змінної $X$. \medskip
    
    Спершу, візьмемо ортоспектральний розклад правої частини,
    
    \begin{equation}
        \rho \left(Z^k - U^k\right) - S = Q \Lambda Q^\intercal,
    \end{equation}
    
    де $\Lambda = \diag (\lambda_1, \ldots, \lambda_n)$, а $Q^\intercal Q = Q Q^\intercal = I$. Множимо \eqref{eq:6.5} на $Q^\intercal$ зліва і на $Q$ справа, отримуємо
    
    \begin{equation}
        \rho \tilde X - \tilde x^{-1} = \Lambda,
    \end{equation}
    
    де $\tilde X = Q^\intercal X Q$. \medskip
    
    Тепер можемо побудувати діагональний розв'язок цього рівняння, тобто знайти додатні числа $\tilde X_{i,i}$ що задовольняють рівнянню
    
    \begin{equation}
        \rho \tilde X_{i,i} - 1 / \tilde X_{i,i} = \lambda_i.
    \end{equation}
    
    А це квадратне рівняння, тому
    
    \begin{equation}
        \tilde X_{i,i} = \frac{\lambda_i + \sqrt{\lambda_i^2 + 4 \rho}}{2 \rho},
    \end{equation}
    
    яке завжди додатне адже $\rho > 0$. \medskip
    
    Далі лишається обчислити $X = Q \tilde X Q^\intercal$ і отримаємо розв'язок \eqref{eq:6.5}. 
    
    \begin{remark}
        Як наслідок, обчислювальна складність кроку оновлення змінної $X$ приблизно дорівнює складності ортоспектрального розкладу симетричної матриці.
    \end{remark}
\end{proof}
