\subsection{Алгоритм}

ADMM є алгоритмом, що покликаний поєднати можливість декомпозиції у методі двоїстого сходження з покращеною збіжністю методу множників. Алгоритм застосовується до задач у вигляді

\begin{equation}
	\label{eq:3.1}
	f(x) + g(z) \xrightarrow[A x + B z = c]{} \min.
\end{equation}

де змінні $x \in \RR^n$, $z \in \RR^m$, (сталі) матриці  $A \in \RR^{p \times n}$, $B \in \RR^{p \times m}$, (сталий) вектор $c \in \RR^p$. Поки що ми припустимо, що $f$ і $g$ опуклі, а більш конкретні припущення будуть розглянуті в \S3.2. Єдиною відмінністю від загальної задачі \eqref{eq:2.1} з лінійним обмеженням типу рівність є те, що змінна $x$ була розділена на дві частини, $x$ і $z$, причому цільова функція є розділимою відносно такого поділу. Позначатимемо оптимальне значення задачі \eqref{eq:3.1} так:

\begin{equation}
	p^* = \inf \{ f(x) + g(z) | A x + B z = c \}.
\end{equation}

Як і у методі множників, розглянемо розширену функцію Лагранжа:

\begin{equation}
	L_\rho (x, z, y) = 
	f(x) + g(z) + y^\intercal \cdot (A x + B z - c) + 
	(\rho / 2) \cdot \| A x + B z - c \|_2^2.
\end{equation}


ADMM складаєть з ітерацій

\begin{align}
	\label{eq:3.2}
	x^{k + 1} &\coloneqq \Argmin_x L_\rho \left( x, z^k, y^k \right), \\
	\label{eq:3.3}
	z^{k + 1} &\coloneqq \Argmin_z L_\rho \left( x^{k + 1}, z, y^k \right), \\
	\label{eq:3.4}
	y^{k + 1} &\coloneqq y^k + 
	\rho \cdot \left( A x^{k + 1} + B z^{k + 1} - c \right),
\end{align}

де $\rho > 0$. Алгоритм дуже схожий на двоїсте сходження і на метод множників: він складається з кроку \eqref{eq:3.2} мінімізації по $x$, кроку \eqref{eq:3.3} мінімізації по $z$, і кроку \eqref{eq:3.4} оновлення двоїстої змінної. Як і у методі множників, при оновленні двоїстої змінної у якості розміру кроку використовується штрафний параметр $\rho$. \medskip

Метод множників для задачі \eqref{eq:3.1} мав форму

\begin{align}
	\left( x^{k + 1}, z^{k + 1} \right) &\coloneqq 
	\Argmin_{x, z} L_\rho \left( x, z, y^k \right), \\
	y^{k + 1} &\coloneqq y^k + 
	\rho \cdot \left( A x^{k + 1} + B z^{k + 1} - c \right).
\end{align}

Тут розширена функція Лагранжа мінімізувалася по $x$ і по $z$ водночас.  Натомість у ADMM змінні $x$ та $z$ оновлюються по черзі (eng. \textit{in the alternating fashion}), звідки і походить назва алгоритму. ADMM можна розглядати як звичайний метод множників де замість одночасної мінімізації використовується один прохід Гауса-Зейделя по $x$ і $z$ \cite[\S10.1]{90}. Виявляється, що розділення мінімізації по $x$ і $z$ на два кроки це саме те, що дозволяє застосувати декомпозицію у випадку коли $f$ або $g$ розділимі. \medskip

\begin{remark}
    ``Стан'' алгоритму складається тільки з $z^k$ і $y^k$, тобто $\left( z^{k + 1}, y^{k + 1} \right)$ є функцією від $\left( z^k, y^k \right)$, а змінна $x^k$ є радше проміжним результатом обчислень.
\end{remark}

Зрозуміло, що якщо поміняти місцями (пере-назвати) $x$ і $z$, $f$ і $g$, $A$ і $B$ у задачі \eqref{eq:3.1}, то ми отримаємо той же алгоритм з інвертованим порядком кроків \eqref{eq:3.2} і \eqref{eq:3.3}. Тут ролі $x$ і $z$ майже симетричні, єдине що при інвертованому порядку вийде нібито крок оновлення двоїстої змінної $y$ знаходиться між кроками оновлення $x$ та $z$.

\subsubsection{Масштабована форма}

ADMM можна записати трохи інакше, у вигляді який зачасту є більш зручним. Для цього поєднаємо лінійний і квадратичний доданок у розширеній функції Лагранжа і масштабуємо двоїсту змінну. Якщо позначити нев'язку $r = A x + B z - c$, то будемо мати

\begin{align}
	y^\intercal \cdot r + (\rho / 2) \cdot \|r\|_2^2 &= (\rho / 2) \cdot \| r + (1 / \rho) \cdot y \|_2^2 - (1/2\rho) \cdot \|y\|_2^2 = \\
	 &= (\rho / 2) \cdot \| r + u \|_2^2 - (\rho/2) \cdot \|u\|_2^2,
\end{align}

де $u = (1 / \rho) \cdot y$ називається \textit{масштабованою двоїстою змінною}. За допомогою масштабованої двоїстої змінної ADMM можна записати у вигляді

\begin{align}
	\label{eq:3.5}
	x^{k + 1} &\coloneqq \Argmin_x \left( f(x) + (\rho / 2) \cdot \left\| A x + B z^k - c + u^k \right\|_2^2 \right), \\
	\label{eq:3.6}
	z^{k + 1} &\coloneqq \Argmin_z \left( g(z) + (\rho / 2) \cdot \left\| A x^{k + 1} + B z - c + u^k \right\|_2^2 \right), \\
	\label{eq:3.7}
	u^{k + 1} &\coloneqq u^k + A x^{k + 1} + B z^{k + 1} - c.
\end{align}

Якщо позначити нев'язку на ітерації $k$ як $r^k = A x^k + B z^k - c$, то побачимо, що

\begin{equation}
	u^k = u^0 + \Sum_{j = 1}^k r^j,
\end{equation}

тобто є сумою нев'язок.

\begin{definition}
    Першу форму ADMM яка складається з ітерацій \eqref{eq:3.2}--\eqref{eq:3.4} будемо називати \textit{немасштабованою} формою, а другу форму яка складається з ітерацій \eqref{eq:3.5}--\eqref{eq:3.7} --- \textit{масштабованою}, оскільки вона використовує масштабовану двоїсту змінну.
\end{definition}

Ці форми, очевидно, еквівалентні, але формули у масштабованій формі задачі коротші ніж в немасштабованій, тому ми будемо використовувати саме масштабовану форму надалі. Немасштабована форма застосовуватиметься тільки коли нам знадобиться інтерпретація зав'язана на немасштабовану двоїсту змінну.

\subsection{Збіжність}

Є багато результатів стосовно збіжності ADMM; тут ми обмежимося базовим, але все ще дуже загальним результатом, який застосовний до усіх прикладів які ми розглянемо. Зробимо одне припущення щодо функцій $f$ і $g$ і ще одне припущення щодо задачі \eqref{eq:3.1}.

\begin{assumption}
	Функції $f: \RR^n \to \RR \cup \{ + \infty \}$ і $g: \RR^m \to \RR \cup \{ + \infty \}$ замкнуті, правильні, та опуклі.
\end{assumption}

Це припущення можна компактно записати в термінах над-графіків: функція $f$ задовольняє припущення тоді і тільки тоді, коли її над-графік

\begin{equation}
	\epigraph f = \left\{ (x, t) \in \RR^n \times \RR \middle| f(x) \le t \right\}
\end{equation}

є замкнутою непорожньою опуклою множиною. 

\begin{remark}
    Це припущення означає, що підзадачі які виникають у кроках \eqref{eq:3.2} і \eqref{eq:3.3} \textit{можна розв'язати}, тобто існують $x$ і $z$ (не обов'язково єдині без подальших припущень щодо $A$ і $B$) які мінімізують розширену функцію Лагранжа.
\end{remark}

\begin{remark}
    Важливо, що це припущення не вимагає від $f$ і $g$ диференційовності чи скінченності.
\end{remark}

\begin{example}
    Зокрема, $f$ може бути індикатором непорожньої опуклої множини $\mathcal{C}$, тобто $f(x) = 0$ для $x \in \mathcal{C}$ і $f(x) = + \infty$ інакше. У цьому випадку крок \eqref{eq:3.2} мінімізації по $x$ буде задачею квадратичного програмування над $\mathcal{C}$.
\end{example}

\begin{assumption}
	У не розширеної функції Лагранжа $L_0$ є сідлова точка.
\end{assumption}

Тобто, існують $\left( x^*, z^*, y^* \right)$, не обов'язково єдині, такі, що
\begin{equation}
	L_0 \left( x^*, z^*, y \right) \le L_0 \left( x^*, z^*, y^* \right) \le L_0 \left( x, z, y^* \right)
\end{equation}
виконується для усіх $x$, $z$, $y$. \medskip

З першого припущення випливає, що $L_0 \left( x^*, z^*, y^* \right)$ є скінченним для довільної сідлової точки $\left( x^*, z^*, y^* \right)$. З цього у свою чергу випливає, що $ \left( x^*, z^* \right)$ є розв'язком \eqref{eq:3.1}, тобто що $A x^* + B z^* = c$ і $f \left( x^* \right), g \left( z^* \right) < \infty$. Звідси також випливає, що $y^*$ є двоїстою оптимальною точкою, і що оптимальні значення прямої і двоїстої задач однакові, тобто що виконується строга двоїстість. 

\begin{remark}
    Ми не робили жодних більш конкретних припущень щодо $A$, $B$ чи $c$. Зокрема, ані $A$ ані $B$ не зобов'язані бути невиродженими. 
\end{remark}

\subsubsection{Збіжність}

\begin{theorem}
	При виконанні припущень 1 і 2, ітерації ADMM задовольняють наступне:
	\begin{itemize}
		\item \textit{Збіжність нев'язки}. $r^k \to 0$ при $k \to \infty$, тобто ітерації збігаються до допустимої множини.
		\item \textit{Збіжність цільової функції}. $f \left( x^k \right) + g \left( z^k \right) \to p^*$ при $k \to \infty$, тобто цільова функція збігається до оптимального значення.
		\item \textit{Збіжність двоїстої змінної}. $y^k \to y$ при $k \to \infty$, де $y^*$ є двоїстою оптимальною точкою.
	\end{itemize}
\end{theorem}

Доведення перших двох пунктів подані у додатку. 

\begin{remark}
    $x^k$ і $z^k$ не зобов'язані збігатися до оптимальних значень, хоча це можна забезпечити шляхом подальших припущень.
\end{remark}

\subsubsection{Збіжність на практиці}

Прості приклади показують, що ADMM може дуже повільно збігатися до високої точності. Щоправда, здебільшого ADMM дуже швидко сходиться (за кілька десятків ітерацій) до пристойної точності, а це саме те, що потрібно у більшості застосувань. Така поведінка робить ADMM схожим на алгоритми на кшталт двоїстого градієнтного методу у тому сенсі, що вони швидко видають пристойний результат який можна застосовувати на практиці. Щоправда повільна збіжність ADMM відрізняє його від методу Ньютона (або, що задач з обмеженнями, методів внутрішньої точки), де навіть висока точність може бути досягнута за розумний час. у деяких випадках можливо поєднати ADMM з одним із таких методів синтезу високоточного розв'язку з менш точного розв'язку \cite{64}, але здебільшого ADMM застосовується у галузях де пристойної точності цілком достатньо. На щастя, саме так воно і є для задач великого масштабу які ми розглядаємо. Ба більше, у задачах статистики або машинного навчання, надточне знаходження параметрів здебільшого тягне за собою або незначне покращення якості прогнозів (головної метрики якості), або взагалі жодного покращення.

\subsection{Умови оптимальності і критерій зупинки}

\begin{lemma}
    Необхідними і достатніми умовами оптимальності є пряма допустимість
    
    \begin{equation}
    	\label{eq:3.8}
    	A x^* + B z^* - c = 0,
    \end{equation}
    
    і двоїста допустимість,
    
    \begin{align}
    	\label{eq:3.9}
    	0 &\in \partial f \left( x^* \right) + A^\intercal \cdot y^*, \\
    	\label{eq:3.10}
    	0 &\in \partial g \left( z^* \right) + B^\intercal \cdot y^*.
    \end{align}
\end{lemma}

\begin{remark}
    Тут $\partial$ позначає оператор субдиференціювання, див. \cite{140, 19, 99} для визначення. 
\end{remark}

\begin{remark}
    Коли $f$ і $g$ диференційовні, субдиференціали $\partial f$ і $\partial g$ можна замінити на градієнти $\nabla f$ і $\nabla g$, а $\in$ --- на $=$.
\end{remark}

\begin{proof}
    Оскільки $z^{k + 1}$ мінімізує $L_\rho \left( x^{k + 1}, z, y^k \right)$ за визначенням, то ми маємо
    
    \begin{align}
    	0 &\in  \partial g \left( z^{k + 1} \right) + B^\intercal \cdot y^k + \rho \cdot B^\intercal \cdot \left( A x^{k + 1} + B z^{k + 1} - c \right) = \\
    	&= \partial g \left( z^{k + 1} \right) + B^\intercal \cdot y^k + \rho \cdot B^\intercal \cdot r^{k + 1} = \\
    	&= \partial g \left( z^{k + 1} \right) + B^\intercal \cdot y^{k + 1}.
    \end{align}
    
    Це означає, що $z^{k + 1}$ і $y^{k + 1}$ завжди задовольняють \eqref{eq:3.10}, тобто для оптимальності залишається забезпечити \eqref{eq:3.8} і \eqref{eq:3.9}. Цей феномен аналогічний до того, що ітерації методу множників завжди є двоїсто допустимими, див.~\ref{section-02}. \medskip
    
    Оскільки $x^{k + 1}$ мінімізує $L_\rho \left( x, z^k, y^k \right)$ за визначенням, то ми маємо
    
    \begin{align}
    	0 &\in \partial f \left( x^{k + 1} \right) + A^\intercal \cdot y^k + \rho \cdot A^\intercal \cdot \left( A x^{k + 1} + B z^k - c \right) = \\
    	&= \partial f \left( x^{k + 1} \right) + A^\intercal \cdot \left( y^k + \rho \cdot r^{k + 1} + \rho \cdot B \cdot \left( z^k - z^{k + 1} \right)\right) = \\
    	&= \partial f \left( x^{k + 1} \right) + A^\intercal \cdot y^{k + 1} + \rho \cdot A^\intercal \cdot B \cdot \left( z^k - z^{k + 1} \right),
    \end{align}
    
    або, що те саме,
    
    \begin{equation}
    	\rho \cdot A^\intercal \cdot B \cdot \left( z^{k + 1} - z^k \right) \in \partial f \left( x^{k + 1} \right) + A^\intercal \cdot y^{k + 1}.
    \end{equation}
    
    Це у свою чергу означає, що 
    
    \begin{equation}
    	s^{k + 1} = \rho \cdot A^\intercal \cdot B \cdot \left( z^{k + 1} - z^k \right)
    \end{equation}
    
    можна розглядати як нев'язку до умови двоїстої допустимості \eqref{eq:3.9}.
\end{proof}

\begin{definition}
    Будемо називати $s^{k + 1}$ \textit{двоїстою нев'язкою} на ітерації $k + 1$, а $r^{k + 1} = A x^{k + 1} + B z^{k + 1} - c$ --- \textit{прямою нев'язкою'} на ітерації $k + 1$.
\end{definition}

Резюмуючи, умови оптимальності для ADMM складаються з трьох умов \eqref{eq:3.8}-\eqref{eq:3.10}. Остання умова \eqref{eq:3.10} завжди виконується для трійки $ \left( x^{k + 1}, z^{k + 1}, y^{k + 1} \right)$; нев'язки інших двох, \eqref{eq:3.8} та \eqref{eq:3.9} є прямою та двоїстою нев'язками $r^{k + 1}$ та $s^{k + 1}$ відповідно. Ці дві нев'язки збігаються\footnote{Насправді, доведення у додатку показує, що $B \cdot \left( z^{k + 1} - z^k \right)$ збігається до нуля, звідки випливає що $s^k$ збігається до нуля.} до нуля при ітераціях ADMM.

\subsubsection{Критерій зупинки}

Можна показати, що нев'язки умов оптимальності пов'язані з оцінкою на нестачу до оптимальності цільової функції, тобто на $f \left( x^k \right) + g \left( z^k \right) - p^*$. Як показано у додатку, виконується наступна нерівність:

\begin{equation}
	\label{eq:3.11}
	f \left( x^k \right) + g \left( z^k \right) - p^* \le - \left( y^k \right)^\intercal \cdot r^k + \left( x^k - x^* \right)^\intercal \cdot s^k.
\end{equation}

Звідси випливає, що коли нев'язки $r^k$ і $s^k$ достатньо малі, то нестача до оптимальності також мала. \medskip

Щоправда, ми не можемо користуватися безпосередньо цією нерівністю як критерієм зупинки, адже ми не знаємо $x^*$. Втім, якщо ми тим чи іншим чином знаємо, що $\left\| x^k - x^* \right\|_2 \le d$, то можна записати

\begin{equation}
	f \left( x^k \right) + g \left( z^k \right) - p^* \le - \left( y^k \right)^\intercal \cdot r^k + d \cdot \left\|s^k\right\|_2 \le \left\|y^k\right\|_2 \cdot \left\|r^k\right\|_2 + d \cdot \left\|s^k\right\|_2.
\end{equation}

Праву (або середню) частину цієї нерівності можна використовувати як оцінку на нестачу до оптимальності. \medskip

Звідси випливає наступний логічний критерій зупинки: пряма і двоїста нев'язки мають бути малі, а саме

\begin{equation}
	\label{eq:3.12}
	\left\| r^k \right\|_2 \le \epsilon^{\text{pri}} \quad \text{та} \quad \left\|s^k\right\|_2 \le \epsilon^{\text{dual}},
\end{equation}

де $\epsilon^{\text{pri}} > 0$ і $\epsilon^{\text{dual}} > 0$ є так званими толерантностям до умов прямої і двоїстої (не) допустимості, \eqref{eq:3.8} і \eqref{eq:3.9} відповідно. Ці толерантності можна вибрати за допомогою абсолютних або відносних критеріїв, наприклад таких:

\begin{align}
	\epsilon^{\text{pri}} &= \sqrt{p} \cdot \epsilon^{\text{abs}} + \epsilon^{\text{rel}} \cdot \max \left\{ \left\| A x^k \right\|_2, \left\| B z^k \right\|_2, \left\|c\right\|_2 \right\}, \\
	\epsilon^{\text{dual}} &= \sqrt{n} \cdot \epsilon^{\text{abs}} + \epsilon^{\text{rel}} \cdot \left\| A^\intercal \cdot y^k\right\|_2,
\end{align}

де $\epsilon^{\text{abs}} > 0$ є абсолютною\footnote{Множники $\sqrt{p}$ і $\sqrt{n}$ додаються бо $\ell_2$-норми беруть в різних просторах, $\RR^p$ і $\RR^n$  відповідно.} толерантністю, а $\epsilon^{\text{rel}} > 0$ --- відносною. Розумним вибором відносної толерантності є $\epsilon^{\text{rel}} = 10^{-3}$, або $10^{-4}$, залежно від задачі. Вибір абсолютної толерантності сильно залежить від масштабу компонент змінних що розглядаються.

\subsection{Узагальнення і варіації}

Багато варіацій класичного алгоритму ADMM були досліджені протягом десятиліть. Тут ми коротко оглянемо деякі з цих варіацій згруповані за ідеями. Деяких з цих варіацій мають кращу збіжність на практиці ніж класичний ADMM. Більшість узагальнень дуже детально досліджені і для них доведені аналогічні результати стосовно збіжності.

\subsubsection{Змінний штрафний параметр}

Стандартним узагальненням є можливість змінювати штрафний параметр $\rho^k$ на кожній ітерацій, аби покращити збіжність на практиці, а також зменшити вплив початкового значення параметру на перебіг алгоритму. У контексті методу множників цій підхід проаналізований в \cite{142}, де показується, що над-лінійна збіжність може бути досягнута якщо $\rho^k \to \infty$. \medskip

Взагалі кажучи, складно довести збіжність ADMM якщо $\rho$ може змінюватися на кожній ітерації, але вищезгадані теоретичні результати, очевидно, залишаються в силі, якщо припустити що $\rho$ стає сталим після певної скінченної кількості ітерацій. \medskip

Простою схемою яка часто працює добре (див., наприклад \cite{96, 169}) є:

\begin{equation}
	\label{eq:3.13}
	\rho^{k + 1} \coloneqq \begin{cases} \tau^{\text{incr}} \cdot \rho^k, & \left\|r^k\right\|_2 \ge \mu \cdot \left\|s^k\right\|_2, \\ \rho^k / \tau^{\text{decr}}, & \left\|s^k\right\|_2 \ge \mu \cdot \left\|r^k\right\|_2, \\ \rho^k, & \text{інакше}, \end{cases}
\end{equation}

де $\mu > 1$, $\tau^{\text{incr}} > 1$, $\tau^{\text{decr}} > 1$ --- параметри. Типовим вибором може бути $\mu = 10$, $\tau^{\text{incr}} = \tau^{\text{decr}} = 2$. За такою зміною штрафного параметру ховається ідея ``намагатися втримати пряму і двоїсту нев'язку близькими одна до одної по мірі того як обидві збігаються до нуля''. \medskip

Справді, ітераційні рівняння ADMM показують, що великі значення $\rho$ збільшують штраф за пряму нев'язку, тому призводять до малих прямих нев'язок. Натомість визначення $s^{k + 1}$ показує, що малі значення $\rho$ проводять до малих двоїстих нев'язок (але більших прямих нев'язок). \medskip

Схема \eqref{eq:3.13} збільшує $\rho$ у $\tau^{\text{incr}}$ разів коли пряма нев'язка стає великою у порівняння з двоїстою, і зменшує у $\tau^{\text{decr}}$ разів коли відбувається зворотне. Цю схему можна модифікувати із використанням $\epsilon^{\text{pri}}$ та $\epsilon^{\text{dual}}$:

\begin{equation}
	\rho^{k + 1} \coloneqq \begin{cases} \tau^{\text{incr}} \cdot \rho^k, & \epsilon^{\text{pri}} \ge \mu \cdot \epsilon^{\text{dual}}, \\ \rho^k / \tau^{\text{decr}}, & \epsilon^{\text{dual}} \ge \mu \cdot \epsilon^{\text{pri}}, \\ \rho^k, & \text{інакше}, \end{cases}
\end{equation}

\begin{remark}
    Якщо ми використовуємо змінний штрафний параметр то у масштабованій формі ADMM потрібно відповідним чином змінювати масштабовану двоїсту змінну $u^k = (1 / \rho) y^k$.
\end{remark}

\begin{example}
    Якщо $\rho$ зменшилося вдвічі, то $u^k$ потрібно збільшити удвічі перед тим як переходити до наступної ітерації.
\end{example}

\subsubsection{Штрафні доданки загального вигляду}

Іншою ідеєю є заміна квадратичного доданку $(\rho/2) \cdot \|r\|_2^2$ на $(1/2) \cdot r^\intercal P r$, де $P$ --- симетрична додатно визначена матриця. Коли $P$ є сталою то це узагальнення ADMM можна розглядати як класичний ADMM застосований до модифікованої початкової задачі у якій умову $r = 0$ замінили на $F r = 0$, де $F^\intercal F = P$.

\subsubsection{Над-релаксація}

У кроках оновлення змінних $z$ та $y$, величину $Ax^{k + 1}$ можна замінити величиною

\begin{equation}
	\alpha^k A x^{k + 1} - (1 - \alpha^k) \cdot (B z^k - c),
\end{equation}

де $\alpha^k \in (0, 2)$ є \textit{параметром релаксації}.

\begin{definition}
    Коли $\alpha^k > 1$ ця техніка називається \textit{над-релаксацією}, а коли $\alpha^k < 1$ то недо-релаксація.
\end{definition}

Ця схема проаналізована в \cite{63}, і експерименти в \cite{59, 64} дають підстави вважати, що над-релаксація з $\alpha^k \in [1.5, 1.8]$ може покращувати збіжність.

\subsubsection{Неточна мінімізація}

ADMM буде збігатися навіть якщо кроки мінімізації по $x$ і по $z$ виконуються не точно а наближено, за умови що нестачі до оптимальності по всім ітераціям будуть задовольняти певним умовам, наприклад якщо ряд з них буде збіжним. \medskip

Цей результат встановлений Екштейном і Берцекасом в \cite{63}, на основі попередніх результатів Гольштейна і Третьякова \cite{89}. Ця техніка важлива у ситуаціях коли кроки мінімізації по $x$ і по $z$ виконуються за допомогою ітеративних алгоритмів; вона дозволяє розв'язувати такі задачі мінімізації з низькою точністю на перших ітераціях і поступово її підвищувати.

\subsubsection{Порядок кроків}

Декілька варіацій ADMM передбачають зміну порядку кроків оновлення змінних $x$, $y$ і $z$, або виконання одного з кроків декілька разів за ітерацію.

\begin{example}
    Див.~\cite{146}: якщо ми розділимо змінні $x$ і $z$ на $k$ блоків, кожен з яких будемо оновлювати по черзі, перед тим як оновити змінну $y$, то отриманий алгоритм можна інтерпретувати як декілька проходів Гауса-Зейделя замість одного.
\end{example}

\begin{example}
    Якщо ж змінні $x$ та $z$ оновлюються багато разів по черзі перед оновленням $y$ то отримаємо алгоритм, дуже схожий на метод множників (див. \cite[\S3.4.4]{17} для пояснення чому).
\end{example}

\begin{example}
    Іншим варіантом є проведення додаткового кроку оновлення змінної $y$ між кроками для $x$ і $z$, з половинною довжиною крока \cite{17}.
\end{example}

\subsubsection{Пов'язані алгоритми}

Є ще багато інших алгоритмів які відрізняються від ADMM, але які використовують схожі ідеї. Наприклад, Фукушіма \cite{80} застосовує ADMM до постановки двоїсто задачі як до прямої, що призводить до ``двоїстого ADMM'', який, як показано в \cite{65}, є еквівалентним до ``прямого методу Дугласа-Рашфорда'' у постановці \cite[\S3.5.6]{57}. \medskip
% Іншим прикладом є Жу і ко. \cite{183} які обговорюють варіації розподіленого ADMM (описані в \ref{section-07}, \ref{section-08} та \ref{section-10}) такі, що можуть впоратися з різними факторами що ускладнюють, такими як шум при обміні повідомленнями про оновлення, асинхронними оновленнями та ін., що може бути важливо у нестійких обчислювальних системах. 

Є також алгоритми які поєднують ADMM з проксимальним методом множників \cite{141}, радше ніж зі звичайним методом множників; див. також \cite{33, 60}. Інші відповідні публікації включають \cite{62, 143, 59, 147, 158, 159, 42}.

\subsection{Примітки та посилання}

Спочатку ADMM був запропонований в середині 1970-х років Гловінські, Маррокко \cite{86} і Габаєм і Мерсьєром \cite{82}. Є ще ряд інших важливих робіт з аналізу властивостей алгоритму, в тому числі \cite{76, 81, 75, 87, 157, 80, 65, 33}. Зокрема, збіжність ADMM була досліджена багатьма авторами, включаючи Габая \cite{81}, Екштейна і Берцекаса \cite{63}. \medskip

ADMM також застосовувався до ряду статистичних проблем, такі, як розріджена регресія з обмеженнями \cite{18}, розріджене відновлення сигналу \cite{70}, відновлення зображень і знешумлення \cite{72, 154, 134}, мну з регуляризацією нормою сліду \cite{174}, оцінка матриці коваріації з розрідженою оберненою \cite{178}, селектор Данціга \cite{116} і машина опорних векторів \cite{74}. Приклади обробки сигналів див. \cite{42, 40, 41, 150, 149} і посилання в них. \medskip

Багато робіт, що аналізують ADMM, роблять це з точки зору \textit{максимальних монотонних операторів} \cite{23, 141, 142, 63, 144}. Широке різноманіття задач можна задати як знаходження нуля максимального монотонного оператор.

\begin{example}
    Якщо $f$ замкнений, власний і опуклий, то субдиференціальний оператор $\partial f $ є максимальним монотонним, і знаходження нуля $ \partial f $ --- те саме що проста мінімізація $ f $.
\end{example}

\begin{remark}
    Така мінімізація може неявно містити обмеження, якщо $ f $ дозволено приймати значення $ + \infty$.
\end{remark}

\textit{Метод проксимальної точки} Рокфеллера \cite{142} є загальним методом знаходження нуля максимального монотонного оператора, і широкий спектр алгоритмів був розроблений для цієї задачі, включаючи проксимальну мінімізацію (див.~\S4.1), метод множників і ADMM. Для більш детального огляду старої літератури, див.~\cite[\S2]{57}. \medskip

Показано, що метод множників є особливим випадком алгоритму проксимальної точки Рокафеллара \cite{141}. Габай \cite{81} показав, що ADMM --- це особливий випадок методу, що називається \textit{розбиттям Дугласа-Рашфорда} для монотонних операторів \cite{53, 112}, Екштейн та Берцекас \cite{63} показали, що розбиття Дугласа-Ращфорда є особливим\footnote{Натомість варіант ADMM, який виконує додатковий крок оновлення змінної $y$ між кроками оновлення $x$ і $z$ є еквівалентним  до \textit{розбиття Пісмана-Рашфорда} \cite{137, 112} цього, як показали Гловінські і Ле Таллек \cite{87}.} випадком алгоритму проксимальної точки. \medskip

Використовуючи той же фреймворк, Екштейн і Берцкас \cite{63} також показали зв'язок з деякими іншими алгоритмами, такі як метод часткових обернених Спінгарна \cite{153}. Лоуренс і Спінгарн \cite{108} розробили альтернативний фреймворк який показує, що розщеплення Дугласа-Рашфорда, а отже і ADMM, є особливим випадком алгоритму проксимальної  точки; Екштейн і Ферріс \cite{64} пропонують більш свіжий опис, який пояснює цей підхід. \medskip

Головне значення цих результатів полягає в тому, що вони дозволяють застосовувати потужну теорію збіжності методу проксимальної точки до ADMM та інших методів, і показують, що багато з цих алгоритмів по суті ідентичні.

\begin{remark}
    Наші докази збіжності базового алгоритму ADMM, наведені в додатку A, самостійні і не покладаються на цю абстрактну техніку.
\end{remark}

Дослідження методів розбиття операторів та їхній зв'язок із алгоритмами декомпозиції продовжується до сьогодні \cite{66, 67}. \medskip

Значну кількість останніх досліджень розглядають заміну квадратичного штрафного доданку у стандартному методі множників на більш загальний доданок, наприклад, такий, що отримується з \textit{дивергенції Брегмана} \cite{30, 58}; див.~\cite{22} для визначення. \medskip

На жаль, ці узагальнення, здається, не переносяться простим чином від не декомпозиційних методів з розширеною функцією Лагранжа на ADMM: На сьогодні не існує доведення збіжності ADMM з не квадратичними штрафними доданками.